{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-5sJFXFIrP4"
      },
      "outputs": [],
      "source": [
        "DO_TRAIN = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGlwv2ir4n1b"
      },
      "outputs": [],
      "source": [
        "!pip install spacy spacy-transformers razdel datasets seqeval transformers[torch] torch accelerate==0.21.0\n",
        "!python -m spacy download ru_core_news_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ainEbZ-HNBIP"
      },
      "outputs": [],
      "source": [
        "# Загрузка обучающего датасета.\n",
        "import pandas as pd\n",
        "import json\n",
        "data = pd.read_csv(\"ner_data_train.csv\")\n",
        "data_clean = data.copy()\n",
        "data_clean['entities'] = data_clean['entities'].apply(lambda l: l.replace('\\,', ',')if isinstance(l, str) else l)\n",
        "data_clean['entities'] = data_clean['entities'].apply(lambda l: l.replace('\\\\\\\\', '\\\\')if isinstance(l, str) else l)\n",
        "data_clean['entities'] = data_clean['entities'].apply(lambda l: '[' + l + ']'if isinstance(l, str) else l)\n",
        "data_clean['entities'] = data_clean['entities'].apply(lambda l: json.loads(l)if isinstance(l, str) else l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcSrL3PljBPK"
      },
      "source": [
        "Формирование датасета для Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8nrtARKMoUx"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "spacy_num_train = 4495\n",
        "all_data=[]\n",
        "for index, row in data_clean.iterrows():\n",
        "  entities = row['entities']\n",
        "  newList = []\n",
        "  for obj in entities:\n",
        "      item = [obj['offset'], obj['offset'] + obj['length'], obj['label']]\n",
        "      newList.append(item)\n",
        "\n",
        "  entity = {\"entities\": newList}\n",
        "  text = row['video_info'].replace('\"', '\\\"')\n",
        "  res = [text, entity]\n",
        "  all_data.append(res)\n",
        "\n",
        "def convert(path, dataset):\n",
        "    nlp = spacy.blank(\"ru\")\n",
        "    db = DocBin()\n",
        "    for text, annot in tqdm(dataset):\n",
        "            doc = nlp.make_doc(text)\n",
        "            ents = []\n",
        "            for start, end, label in annot[\"entities\"]:\n",
        "                span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "                if not span is None:\n",
        "                    ents.append(span)\n",
        "            doc.ents = ents\n",
        "            db.add(doc)\n",
        "    db.to_disk(path)\n",
        "\n",
        "convert(\"train.spacy\", all_data[:spacy_num_train])\n",
        "convert(\"dev.spacy\", all_data[spacy_num_train:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Puarc4PP1m9"
      },
      "outputs": [],
      "source": [
        "# Отключена из-за возможной проблемы с токенизатором\n",
        "#!python -m spacy train \"spacy_deberta-stage-1.cfg\" --output \"spacy_deberta\" --paths.train \"train.spacy\" --paths.dev \"dev.spacy\" --training.eval_frequency 1 --training.max_steps 1 --gpu-id 0\n",
        "#!python -m spacy train \"spacy_deberta-stage-2.cfg\" --output \"spacy_deberta\" --paths.train \"train.spacy\" --paths.dev \"dev.spacy\" --training.eval_frequency 1 --training.max_steps 1 --gpu-id 0\n",
        "\n",
        "if DO_TRAIN:\n",
        "  !python -m spacy train \"spacy_multilingual-uncased-stage-1.cfg\" --output \"spacy_multilingual\" --paths.train \"train.spacy\" --paths.dev \"dev.spacy\" --training.eval_frequency 20 --training.max_steps 300 --gpu-id 0\n",
        "  !python -m spacy train \"spacy_multilingual-uncased-stage-2.cfg\" --output \"spacy_multilingual\" --paths.train \"train.spacy\" --paths.dev \"dev.spacy\" --training.eval_frequency 20 --training.max_steps 300 --gpu-id 0\n",
        "\n",
        "  !python -m spacy train \"spacy_rubert-tiny2-stage-1.cfg\" --output \"spacy_rubert\" --paths.train \"train.spacy\" --paths.dev \"dev.spacy\" --training.eval_frequency 20 --training.max_steps 300 --gpu-id 0\n",
        "  !python -m spacy train \"spacy_rubert-tiny2-stage-2.cfg\" --output \"spacy_rubert\" --paths.train \"train.spacy\" --paths.dev \"dev.spacy\" --training.eval_frequency 20 --training.max_steps 300 --gpu-id 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rirgZWPDrzGQ"
      },
      "source": [
        "# Модель из Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvJ1FVwwtn9u"
      },
      "source": [
        "Подготовка датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7Ov2IilrxTQ"
      },
      "outputs": [],
      "source": [
        "from razdel import tokenize\n",
        "def extract_labels(item):\n",
        "    raw_toks = list(tokenize(item['video_info']))\n",
        "    words = [tok.text for tok in raw_toks]\n",
        "    # присвоим для начала каждому слову тег 'О' - тег, означающий отсутствие NER-а\n",
        "    word_labels = ['O'] * len(raw_toks)\n",
        "    char2word = [None] * len(item['video_info'])\n",
        "    # так как NER можем состаять из нескольких слов, то нам нужно сохранить эту инфорцию\n",
        "    for i, word in enumerate(raw_toks):\n",
        "        char2word[word.start:word.stop] = [i] * len(word.text)\n",
        "    labels = item['entities']\n",
        "    if isinstance(labels, dict):\n",
        "        labels = [labels]\n",
        "    if labels is not None:\n",
        "        for e in labels:\n",
        "            if e['label'] != 'не найдено':\n",
        "                e_words = sorted({idx for idx in char2word[e['offset']:e['offset']+e['length']] if idx is not None})\n",
        "                if e_words:\n",
        "                    word_labels[e_words[0]] = 'B-' + e['label']\n",
        "                    for idx in e_words[1:]:\n",
        "                        word_labels[idx] = 'I-' + e['label']\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                continue\n",
        "        return {'tokens': words, 'tags': word_labels}\n",
        "    else: return {'tokens': words, 'tags': word_labels}\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "ner_data = [extract_labels(item) for i, item in data_clean.iterrows()]\n",
        "ner_train, ner_test = train_test_split(ner_data, test_size=0.2, random_state=1)\n",
        "label_list = sorted({label for item in ner_train for label in item['tags']})\n",
        "if 'O' in label_list:\n",
        "    label_list.remove('O')\n",
        "    label_list = ['O'] + label_list\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "ner_data = DatasetDict({\n",
        "    'train': Dataset.from_pandas(pd.DataFrame(ner_train)),\n",
        "    'test': Dataset.from_pandas(pd.DataFrame(ner_test))\n",
        "})\n",
        "ner_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuP8lOSHuRz-"
      },
      "source": [
        "Обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNZhr2BWuUOr"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "model_checkpoint = \"cointegrated/rubert-tiny\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, device='gpu')\n",
        "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples['tags']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        label_ids = [label_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2Rs7jOAxZD-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "base_model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
        "base_model.config.id2label = dict(enumerate(label_list))\n",
        "base_model.config.label2id = {v: k for k, v in base_model.config.id2label.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txQADU2kxY6G"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = load_metric(\"seqeval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC-D2L4axf7C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VwzaJvA8sI4"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_args = TrainingArguments(\n",
        "    \"ner\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=25,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy='no',\n",
        "    report_to='none',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    base_model,\n",
        "    train_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "for param in base_model.parameters():\n",
        "    param.requires_grad = True\n",
        "if DO_TRAIN:\n",
        "  trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZE6088-9xW7"
      },
      "outputs": [],
      "source": [
        "if DO_TRAIN:\n",
        "  trainer.evaluate()\n",
        "  base_model.save_pretrained('base_model_bert')\n",
        "  tokenizer.save_pretrained('base_model_bert')\n",
        "else:\n",
        "  label_list = \\\n",
        "  ['O',\n",
        " 'B-Дата',\n",
        " 'B-бренд',\n",
        " 'B-вид спорта',\n",
        " 'B-видеоигра',\n",
        " 'B-команда',\n",
        " 'B-лига',\n",
        " 'B-локация',\n",
        " 'B-модель',\n",
        " 'B-название проекта',\n",
        " 'B-организация',\n",
        " 'B-персона',\n",
        " 'B-сезон',\n",
        " 'B-серия',\n",
        " 'I-Дата',\n",
        " 'I-бренд',\n",
        " 'I-вид спорта',\n",
        " 'I-видеоигра',\n",
        " 'I-команда',\n",
        " 'I-лига',\n",
        " 'I-локация',\n",
        " 'I-модель',\n",
        " 'I-название проекта',\n",
        " 'I-организация',\n",
        " 'I-персона',\n",
        " 'I-сезон',\n",
        " 'I-серия']\n",
        "  base_model = AutoModelForTokenClassification.from_pretrained('base_model_bert', num_labels=len(label_list))\n",
        "  tokenizer = AutoTokenizer.from_pretrained('base_model_bert', device='gpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7Ey-d3QDsW0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "def get_bio_tags_base(text):\n",
        "    dismissed_token = re.compile(r'\\xad+|\\u200b+')\n",
        "    text = [re.sub(dismissed_token, '[UNK]', tok) for tok in text]\n",
        "    tokens = tokenizer(text, truncation=True, is_split_into_words=True, return_tensors='pt')\n",
        "    words = tokens.word_ids()\n",
        "    tokens = {k: v.to(base_model.device) for k, v in tokens.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = base_model(**tokens)\n",
        "\n",
        "    indices = pred.logits.argmax(dim=-1)[0].cpu().numpy()\n",
        "    labels = []\n",
        "    prev=words[1] # это всегда ноль - первое слово\n",
        "    labels = [label_list[indices[1]]]\n",
        "    for word, tag in zip(words[1:-1], indices[1:-1]):\n",
        "        if word != prev:\n",
        "            labels.append(label_list[tag])\n",
        "            prev=word\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy-aQFCIeaoe"
      },
      "outputs": [],
      "source": [
        "# Т.к. не всегда удается сопоставить токены разных токенизаторов\n",
        "def validate_and_fix_bio(text, bio_tags):\n",
        "  razdel_tokens = list(tokenize(text))\n",
        "  len_razdel = len(razdel_tokens)\n",
        "  len_bio_tags = len(bio_tags)\n",
        "  if len_razdel > len_bio_tags:\n",
        "    for x in range(len_razdel - len_bio_tags):\n",
        "      bio_tags.append('O')\n",
        "  if len_razdel < len_bio_tags:\n",
        "    bio_tags = bio_tags[0: len_razdel]\n",
        "  return bio_tags\n",
        "\n",
        "print(validate_and_fix_bio('Ереван',[]))\n",
        "print(validate_and_fix_bio('',['O']))\n",
        "print(validate_and_fix_bio('Ереван',['O']))\n",
        "print(validate_and_fix_bio('',['']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX6YhTh1mjGz"
      },
      "source": [
        "Подготовка данных для отправки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeZLf8KGlrIt"
      },
      "outputs": [],
      "source": [
        "from spacy.symbols import ORTH\n",
        "\n",
        "# Универсальный метод для получения BIO-тегов при помощи Spacy-модели.\n",
        "def get_bio_tags_spacy(doc):\n",
        "    bio = []\n",
        "    for ent in doc:\n",
        "        cat = ent.ent_iob_\n",
        "        if cat != 'O':\n",
        "            cat = cat + '-' + ent.ent_type_\n",
        "        bio.append(cat)\n",
        "    return bio\n",
        "\n",
        "def submission_spacy(spacy_model):\n",
        "    ner_test_data = pd.read_csv(\"ner_test.csv\")\n",
        "    new_sub = pd.DataFrame(columns=[['video_info', 'entities_prediction']])\n",
        "    for i, elem in ner_test_data.iterrows():\n",
        "        text = elem['video_info']\n",
        "        new_sub.loc[i, 'video_info'] = text\n",
        "        tags = get_bio_tags_spacy(spacy_model(text))\n",
        "        tags = validate_and_fix_bio(text, tags)\n",
        "        new_sub.loc[i, 'entities_prediction'] = str(tags)\n",
        "    return new_sub\n",
        "\n",
        "def submission_base():\n",
        "    ner_test_data = pd.read_csv(\"ner_test.csv\")\n",
        "    new_sub = pd.DataFrame(columns=[['video_info', 'entities_prediction']])\n",
        "    for i, elem in ner_test_data.iterrows():\n",
        "        text = elem['video_info']\n",
        "        new_sub.loc[i, 'video_info'] = text\n",
        "        tags = get_bio_tags_base(text)\n",
        "        tags = validate_and_fix_bio(text, tags)\n",
        "        new_sub.loc[i, 'entities_prediction'] = str(tags)\n",
        "    return new_sub\n",
        "\n",
        "def submission_hybrid():\n",
        "    ner_test_data = pd.read_csv(\"ner_test.csv\")\n",
        "    new_sub = pd.DataFrame(columns=[['video_info', 'entities_prediction']])\n",
        "    for i, elem in ner_test_data.iterrows():\n",
        "        text = elem['video_info']\n",
        "        new_sub.loc[i, 'video_info'] = text\n",
        "\n",
        "        tags_b = get_bio_tags_base(text)\n",
        "        tags_b = validate_and_fix_bio(text, tags_b)\n",
        "\n",
        "        tags_s1 = get_bio_tags_spacy(spacy_multilingual(text))\n",
        "        tags_s1 = validate_and_fix_bio(text, tags_s1)\n",
        "\n",
        "        tags_s2 = get_bio_tags_spacy(spacy_multilingual(text))\n",
        "        tags_s2 = validate_and_fix_bio(text, tags_s2)\n",
        "        hybrid_tags = []\n",
        "        for t1, t2, t3 in zip(tags_b, tags_s1, tags_s2):\n",
        "          t = t1\n",
        "          if t == 'O':\n",
        "            t = t2\n",
        "          if t == 'O':\n",
        "            t = t3\n",
        "          hybrid_tags.append(t)\n",
        "\n",
        "        new_sub.loc[i, 'entities_prediction'] = str(hybrid_tags)\n",
        "    return new_sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwID2d4XGfgc"
      },
      "outputs": [],
      "source": [
        "spacy_multilingual = spacy.load(R\"spacy_multilingual/model-last\")\n",
        "spacy_multilingual.tokenizer.add_special_case(\":>\", [{ORTH: \":\"}, {ORTH: \">\"}])\n",
        "spacy_multilingual_result = submission_spacy(spacy_multilingual)\n",
        "spacy_multilingual_result.to_csv('submission_spacy_multilingual.csv', index=False)\n",
        "\n",
        "spacy_rubert = spacy.load(R\"spacy_rubert/model-last\")\n",
        "spacy_rubert.tokenizer.add_special_case(\":>\", [{ORTH: \":\"}, {ORTH: \">\"}])\n",
        "spacy_rubert = submission_spacy(spacy_rubert)\n",
        "spacy_rubert.to_csv('submission_spacy_rubert.csv', index=False)\n",
        "\n",
        "base_rubert = submission_base()\n",
        "base_rubert.to_csv('submission_base_rubert.csv', index=False)\n",
        "\n",
        "# Из-за бага модели приходится загружать по новой\n",
        "spacy_multilingual = spacy.load(R\"spacy_multilingual/model-last\")\n",
        "spacy_multilingual.tokenizer.add_special_case(\":>\", [{ORTH: \":\"}, {ORTH: \">\"}])\n",
        "spacy_rubert = spacy.load(R\"spacy_rubert/model-last\")\n",
        "spacy_rubert.tokenizer.add_special_case(\":>\", [{ORTH: \":\"}, {ORTH: \">\"}])\n",
        "hybryd = submission_hybrid()\n",
        "hybryd.to_csv('submission_hybryd.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}